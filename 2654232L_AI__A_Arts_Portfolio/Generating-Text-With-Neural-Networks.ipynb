{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1cdf46",
   "metadata": {},
   "source": [
    "# Generating Text with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301f33b",
   "metadata": {},
   "source": [
    "## https://github.com/2654232L/2654232.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e3aee",
   "metadata": {},
   "source": [
    "# Framing the Problem\n",
    "\n",
    "### What does the program do?\n",
    "This program builds a model to predict text based off a Shakespeare text file.\n",
    "\n",
    "### Where would this program be used?\n",
    "This program may be used to predict new text Shakespeare may have written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70325e9",
   "metadata": {},
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390db495",
   "metadata": {},
   "source": [
    "### Getting the Dataset Online and Reading the File\n",
    "- The code below is first importing the tensorflow library and giving it the name of 'tf'. This is important so we do not have to keep typing out 'tensorflow' and can give it a short nickname.\n",
    "- Next, the code below shows retrieving the data from the homl.info/shakespeare website. The code is using keras from tensorflow.\n",
    "- The code is next reading the text from the file (given the nickname f) and putting the text from the file into the variable shakespear_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5298b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4d4eb",
   "metadata": {},
   "source": [
    "### Printing Sections of the Text\n",
    "\n",
    "- The cell below just prints the first 80 characters of the text in the file. For example if you wanted to print the last characters of the text in the file you would code:\n",
    "                   \"print(shakespeare_text[-80:])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shakespeare_text[:80]) # not relevant to machine learning but relevant to exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55b2aa",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753cfbd6",
   "metadata": {},
   "source": [
    "### Text Vectorisation\n",
    "##### (Source used: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)\n",
    "\n",
    "- The cell below will instantisate a TextVectorization in the variable text_vec_layer. This just means any text the text_vec_layer is used on will split it up by character (split=\"character\") and will put all the characters into lowercase (standardize=\"lower\").\n",
    "- text_vec_layer is then used to adapt our Shakespeare text and apply these rules to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7308552",
   "metadata": {},
   "source": [
    "- This cell below will print out the text vectorisation. Each number represents a character in the text. The shape shows how many characters the are. And it shows that the data type of the vector is integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vec_layer([shakespeare_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c4920",
   "metadata": {},
   "source": [
    "- The cell below is just showing us how many tokens there are, which is the number of distinct characters, which in this case is 39.\n",
    "- It is also showing us the total number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c79b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_tokens, dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa39ed",
   "metadata": {},
   "source": [
    "### Preparing Training and Test Data\n",
    "##### (Source used: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices, https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/op/data/WindowDataset https://stackoverflow.com/questions/71211053/what-tensorflows-flat-map-window-batch-does-to-a-dataset-array https://www.w3schools.com/python/ref_random_seed.asp#:~:text=Definition%20and%20Usage,uses%20the%20current%20system%20time. https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)\n",
    "\n",
    "- The cell below is creating a function that takes in a sequence, this will be the characters in the text. It also takes in the length.\n",
    "- The variable ds is created which is splitting up the sequence into slices which will be each of the characters.\n",
    "- Then .window is used which means each window is a dataset that contains a subset of elements from the sequence. It takes the arguments length+1, the shift=1 and drop_remainder is True. The length is how many elements from the dataset to put into a window. The shift means the input elements in the window shift by one. If the drop_remainder is True this means if the last window is smaller than the window size then it is dropped.\n",
    "- Flat_map makes sure the dataset is kept in the same order. The function is used to flatten the data from a dataset of datasets into a dataset of elements.\n",
    "- If shuffle = True then the dataset is shuffled. This works by having 100000 as the buffer size, which is just the amount of data used to shuffle, the seed is also set as equal to seed which is set to make sure the random numbers re the same if you rerun the program.\n",
    "- Dataset is then put into batches of size 32, which means amount of consecutive elements to combine into a single batch. A batch defines the number of samples before updating the model parameters.\n",
    "- Finally, the function returns the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8571a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90edc49",
   "metadata": {},
   "source": [
    "- The cell below simply plugs in the data for the training set, the valid set, and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:100_000], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[100_000:160_000], length=length)\n",
    "test_set = to_dataset(encoded[160_000:], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d65051",
   "metadata": {},
   "source": [
    "# Building and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc972a",
   "metadata": {},
   "source": [
    "##### (Sources used: https://www.tensorflow.org/api_docs/python/tf/keras/Sequential https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#args https://www.educba.com/tensorflow-dense/)\n",
    "\n",
    "- The following code builds the model.\n",
    "- Sequential groups the following layers into a model:\n",
    "    - Embedding turns the integers into 'dense vectors' of a fixed size. input_dim is the size of the vocabulary and in this case is n_tokens. output_dim is the dimension of the 'dense embedding'.\n",
    "    - GRU takes the units ('dimensionality of the output'), 128, and sets the return_sequences to True which is whether to return 'the last output in the sequence or the full sequence.'\n",
    "    - Dense layer is the one that executes the matrix-vector multiplication\n",
    "\n",
    "- Then the model is compiled\n",
    "- Then model_ckpt is created to monitor accuracy and create checkpoints\n",
    "- The history variable is created to fit the data to the model and sets the amountof epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19e4cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "                    callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996457f",
   "metadata": {},
   "source": [
    "- The cell below applies the model to the Shakespeare text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927fee3",
   "metadata": {},
   "source": [
    "# Generating Text\n",
    "\n",
    "- The cell below shows predicting the next character based on the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0581742",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170c7dd",
   "metadata": {},
   "source": [
    "### Defining Functions\n",
    "##### (Sources used: https://www.kaggle.com/code/tirendazacademy/text-generation-with-tensorflow)\n",
    "\n",
    "- Function to predict the next character\n",
    "- Temperature determines the randomness of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09519ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c9475",
   "metadata": {},
   "source": [
    "- Function to predict the next 50 characters of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db78224",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "1. The first example wants to predict the next bit of text at a temperature of 0.01 so the randomness will be small and the text will be more accurate\n",
    "2. The second example is the same but the temperature of the predictions is 1, so will be more random, and will not be as accurate\n",
    "3. The third example the temperature is 100 so the predicted text will not be as accurate as the other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6803df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef204a0",
   "metadata": {},
   "source": [
    "# Evaluation of Results\n",
    "\n",
    "- With the low temperature the model was quite good at predicting text and it somewhat made sense and sounds like something Shakespeare might say.\n",
    "- However, when the temperature started increasing, for example, when it was one, it sort of made sense, but some words were not real words\n",
    "- When the temperature was 100 the text did not make sense at all the characters were very random.\n",
    "\n",
    "- The model would have been more accurate if the training and test data was larger, however, I decreased this so it would be more time efficient.\n",
    "\n",
    "- What kind of data would you be interested in applying it to?\n",
    "    - Instead of applying the model to Shakespeare a different dataset could be used. For example, using a more modern author, like one of my favourite authors to create a short story. Or, using a dataset of different greetings and using the model to predict different greetings.\n",
    "- Why are you interested in doing that?\n",
    "    - I would be interested in using the text of one of my favourite authors, because the results would be something I am more interest in than Shakespeare.\n",
    "- How do you think you would need to transform your content for the computer?\n",
    "    - I would need to gather data for my own dataset.\n",
    "- What ethical concerns you envision in relation to this code or a similar development\n",
    "    - For the Shakespeare text you could expect issues like predicting innapropriate words - which could cause an issue if a young person was using it, or predicting something that has demographic/racial bias because Shakespeare was probably racist so could raise some ethical issues in that regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451cdc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
